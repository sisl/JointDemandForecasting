{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import sklearn as skl\n",
    "import time\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sys.path.append(\"../../../\")\n",
    "import CalibratedTimeseriesModels\n",
    "\n",
    "from CalibratedTimeseriesModels.utils import *\n",
    "from CalibratedTimeseriesModels.models.gnn import *\n",
    "from CalibratedTimeseriesModels.evaluators import ExplicitEvaluator\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "X_orig = torch.load(\"../../../datasets/processed/openEI/X_openei_011_subset_multitask.pt\")\n",
    "Y_orig = torch.load(\"../../../datasets/processed/openEI/Y_openei_011_subset_multitask.pt\")\n",
    "\n",
    "X = X_orig[0,:,:24].unsqueeze(-1).float()\n",
    "Y = Y_orig[0,:,:12].unsqueeze(-1).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1, random_state=1)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, train_size = 0.7/0.9,random_state=1)\n",
    "X_batches, Y_batches = batch(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagonal Covariance Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layers = [40, 40, 40]\n",
    "model = GaussianNeuralNet(1, 24, hidden_layers, 1, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1, loss: -0.1029\n",
      "epoch : 2, loss: -6.9972\n",
      "epoch : 3, loss: -11.8197\n",
      "epoch : 4, loss: -12.4267\n",
      "epoch : 5, loss: -12.8769\n",
      "epoch : 6, loss: -13.3783\n",
      "epoch : 7, loss: -13.8912\n",
      "epoch : 8, loss: -14.3649\n",
      "epoch : 9, loss: -14.6143\n",
      "epoch : 10, loss: -15.1315\n",
      "epoch : 11, loss: -15.0475\n",
      "epoch : 12, loss: -14.8431\n",
      "epoch : 13, loss: -15.3840\n",
      "epoch : 14, loss: -15.3275\n",
      "epoch : 15, loss: -15.6596\n",
      "epoch : 16, loss: -16.0817\n",
      "epoch : 17, loss: -15.7227\n",
      "epoch : 18, loss: -15.7463\n",
      "epoch : 19, loss: -15.8284\n",
      "epoch : 20, loss: -16.1419\n",
      "epoch : 21, loss: -15.8840\n",
      "epoch : 22, loss: -16.0603\n",
      "epoch : 23, loss: -15.9389\n",
      "epoch : 24, loss: -16.1572\n",
      "epoch : 25, loss: -16.3103\n",
      "epoch : 26, loss: -16.5619\n",
      "epoch : 27, loss: -16.5702\n",
      "epoch : 28, loss: -16.8463\n",
      "epoch : 29, loss: -16.7645\n",
      "epoch : 30, loss: -16.8436\n",
      "epoch : 31, loss: -16.9051\n",
      "epoch : 32, loss: -16.7386\n",
      "epoch : 33, loss: -16.8406\n",
      "epoch : 34, loss: -16.8964\n",
      "epoch : 35, loss: -16.7397\n",
      "epoch : 36, loss: -17.1003\n",
      "epoch : 37, loss: -16.8603\n",
      "epoch : 38, loss: -16.9089\n",
      "epoch : 39, loss: -17.1000\n",
      "epoch : 40, loss: -16.9777\n",
      "epoch : 41, loss: -16.9022\n",
      "epoch : 42, loss: -17.0916\n",
      "epoch : 43, loss: -17.0238\n",
      "epoch : 44, loss: -17.1784\n",
      "epoch : 45, loss: -17.2327\n",
      "epoch : 46, loss: -17.3632\n",
      "epoch : 47, loss: -17.0356\n",
      "epoch : 48, loss: -17.5151\n",
      "epoch : 49, loss: -17.3878\n",
      "epoch : 50, loss: -17.2859\n",
      "epoch : 51, loss: -17.4289\n",
      "epoch : 52, loss: -17.4570\n",
      "epoch : 53, loss: -17.5727\n",
      "epoch : 54, loss: -17.2810\n",
      "epoch : 55, loss: -17.4655\n",
      "epoch : 56, loss: -17.3867\n",
      "epoch : 57, loss: -17.3782\n",
      "epoch : 58, loss: -17.7540\n",
      "epoch : 59, loss: -17.4802\n",
      "epoch : 60, loss: -17.8345\n",
      "Learning finished!\n"
     ]
    }
   ],
   "source": [
    "train(model, X_batches, Y_batches, num_epochs=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0367, grad_fn=<MeanBackward0>),\n",
       " tensor([0.0260, 0.0288, 0.0293, 0.0312, 0.0336, 0.0418, 0.0429, 0.0382, 0.0410,\n",
       "         0.0432, 0.0450, 0.0396], grad_fn=<MeanBackward1>),\n",
       " tensor([0.0229, 0.0250, 0.0265, 0.0292, 0.0314, 0.0355, 0.0355, 0.0322, 0.0367,\n",
       "         0.0402, 0.0421, 0.0380], grad_fn=<StdBackward1>))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtest = model(X_test)\n",
    "mape(dtest, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0510, grad_fn=<PowBackward0>),\n",
       " tensor([0.0016, 0.0019, 0.0018, 0.0020, 0.0023, 0.0031, 0.0033, 0.0026, 0.0031,\n",
       "         0.0032, 0.0035, 0.0029], grad_fn=<MeanBackward1>),\n",
       " tensor([0.0032, 0.0049, 0.0047, 0.0052, 0.0059, 0.0056, 0.0061, 0.0044, 0.0056,\n",
       "         0.0062, 0.0070, 0.0054], grad_fn=<StdBackward1>))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse(dtest, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-19.5743, grad_fn=<MeanBackward0>),\n",
       " tensor([-20.3771, -20.0503,  50.1044, -15.6146, -19.4976, -24.5063, -22.0410,\n",
       "         -18.3323, -24.7329, -26.7688, -16.5699, -14.9878, -24.3020, -21.4480,\n",
       "         -19.7465, -16.7804, -24.3132, -17.0713, -19.9366, -25.1299, -21.7302,\n",
       "         -19.5093, -23.2317, -24.4029, -17.4278, -15.3351, -26.3236, -17.0145,\n",
       "         -25.4478, -25.4584, -23.1114, -20.0582, -20.2174, -18.5640, -25.1007,\n",
       "         -21.9937, -10.9783,  26.2981, -21.9779, -23.2254, -13.6814, -24.1219,\n",
       "         -21.6421, -18.6018, -20.0147, -22.4739, -11.7606, -11.5997, -22.1841,\n",
       "         -24.4418, -18.6160, -22.3303, -23.0140, -20.5605, -24.9485, -28.6246,\n",
       "         -15.3084, -20.4541, -15.5021, -21.7190, -14.4454, -26.1499, -20.1200,\n",
       "         -23.3968, -25.5928, -22.2167, -20.0023,  -5.6641, -19.7642, -23.6748,\n",
       "         -18.8556,  -3.2985, -24.3281, -20.1020, -20.0012, -13.6152, -20.1795,\n",
       "         -26.0380,  -3.1119, -22.4194, -17.4729, -23.1181, -24.8369, -27.3120,\n",
       "         -19.7368, -17.2475, -17.3201, -18.6397, -17.2869, -25.3956, -14.9234,\n",
       "         -21.4210, -25.6266, -17.9865, -21.3433, -22.5755, -14.5394, -13.1332,\n",
       "         -16.4208, -24.5436, -23.1426, -22.5875, -21.2693, -28.5804, -17.3214,\n",
       "         -17.1175, -25.1796, -10.0157, -20.4887, -22.8792, -18.9025, -22.7947,\n",
       "         -17.9023, -24.1423, -23.2423, -22.0343, -18.7398, -17.5651, -20.4669,\n",
       "         -25.1615, -24.6984, -15.2594, -27.7262, -22.2048, -25.7720, -20.4173,\n",
       "         -20.7123, -14.8080, -21.5384, -24.3993, -26.9012, -13.0810, -23.0473,\n",
       "         -19.8634, -24.7330, -19.9761, -25.7751, -14.3544, -15.4166, -23.0087,\n",
       "          59.7462, -26.8780, -25.7569, -26.7313, -27.8810, -26.0476, -26.5518,\n",
       "         -13.2356, -20.0254, -20.3421, -24.2201, -11.6571, -20.4517, -19.7643,\n",
       "         -24.6573, -24.7443, -17.8947, -20.5570, -22.0807,  21.4304, -21.0572,\n",
       "         -14.6852, -15.4221, -23.9216, -24.8943, -17.7382, -23.1134, -19.1930,\n",
       "         -20.5017, -17.3103, -22.4603, -16.7070, -24.1692, -17.0783, -21.3055,\n",
       "         -16.1579, -25.0256, -22.8223, -25.1007, -21.0144, -23.0777, -24.6101,\n",
       "         -24.2694, -24.9578, -20.9693, -20.4334, -22.0233, -11.2932, -21.2990,\n",
       "         -26.9180, -24.6715, -23.9515, -26.3592, -20.7717, -25.5354, -22.9287,\n",
       "         -13.8904, -20.7166, -28.5551, -19.0192, -18.6887, -19.4686, -25.9714,\n",
       "         -18.2297, -21.3435, -21.1839, -23.1639,  -6.9609, -15.2303, -23.4489,\n",
       "         -24.4666, -22.2160, -14.4455, -25.4900, -19.2953, -28.6679, -21.9588,\n",
       "         -21.4824, -20.4055, -25.3823, -25.4241, -21.3720, -22.1257, -23.8573,\n",
       "         -21.9085, -23.6314, -24.4101, -17.6015, -20.8033, -25.4401, -15.7875,\n",
       "         -20.0406,   6.8162,   9.8914, -20.9439, -23.6272, -24.8180, -24.1311,\n",
       "         -15.7059, -26.2876, -20.3032, -18.7976, -18.5184, -20.3069, -18.8735,\n",
       "         -23.2687, -21.3468, -24.3080, -20.0178, -25.7569, -18.7658, -24.8503,\n",
       "         -26.3545, -21.8643, -21.5093,  -3.1909, -26.1140, -18.4517, -25.0096,\n",
       "         -26.5412, -20.9651, -21.5287, -20.6226, -24.0088,  -9.6377, -20.1452,\n",
       "         -24.7648, -26.7433, -13.5146, -17.5421, -23.7727, -25.9179, -23.0492,\n",
       "         -20.1079, -26.6194, -24.1883, -24.2733, -22.1648, -25.1710, -20.1193,\n",
       "         -21.9673, -11.3617, -26.1059, -17.8039, -22.8367,  -3.8430, -21.9738,\n",
       "         -28.0738, -13.0244, -21.9403, -23.7274, -27.8145, -26.9830, -18.1938,\n",
       "         -24.9701, -18.9971, -13.3755,  -5.5368, -21.5875, -19.0967, -22.3370,\n",
       "         -21.9973, -22.4438, -18.9151, -23.4836, -13.3443, -17.0665, -25.4859,\n",
       "         -23.7241, -17.5922, -24.4277, -15.0373, -19.1144, -15.1112, -25.9947,\n",
       "         -22.0766,   8.0230, -19.9128, -13.1372, -15.8008, -19.7768, -19.9022,\n",
       "          -5.2972, -20.6557, -21.8746, -16.5138, -25.1007, -26.7976,  35.8137,\n",
       "         -22.7230, -21.0242, -12.8055, -17.1890,  -7.6257, -24.2366, -18.8599,\n",
       "         -24.9467, -17.4492, -23.5488, -18.5311, -26.3606, -16.7229, -22.1920,\n",
       "         -22.8734, -17.4844, -16.5403, -22.3357, -15.2145, -24.8493, -17.0383,\n",
       "         -27.7418, -20.5842, -25.0308, -23.7369, -22.0504, -23.3759, -23.0366,\n",
       "         -20.6816, -26.3277, -21.5084, -10.6266, -21.3688, -20.3681, -19.3596,\n",
       "         -23.5864,  -8.3665, -22.9659, -19.4999, -24.5221, -10.8811, -19.4297,\n",
       "         -17.3804,  -7.7098, -20.1017, -15.9425, -28.5002, -23.5608, -14.2555,\n",
       "         -21.1539, -18.1605, -22.2639, -24.5426, -23.2081, -18.7003, -25.6328,\n",
       "         -12.0980, -17.5608, -20.6919, -16.9261, -24.1282, -20.1479, -24.4888,\n",
       "         -15.7746, -17.9205, -26.1140, -20.1649,  -1.0072, -16.2952, -21.1029,\n",
       "         -16.6284, -22.3213, -16.9915, -17.0514, -20.2793, -19.4099, -12.7288,\n",
       "         -17.7178, -26.4209, -24.8683, -18.7564, -18.8086, -14.6885, -22.7902,\n",
       "         -18.4705, -18.1658, -20.2165, -22.7906, -20.4336, -15.1241, -26.0073,\n",
       "         -25.0643, -11.6100, -18.7361, -19.9755, -15.5730, -21.8674, -25.3516,\n",
       "         -16.2943, -17.6145, -20.0011, -21.7472, -13.4479, -26.7852, -17.4269,\n",
       "          -9.7365, -20.9654, -12.9504, -21.6749, -25.1099, -14.9346, -25.5266,\n",
       "         -21.3042,  -9.1630, -16.1557, -17.3188, -19.4353, -24.8825, -17.8524,\n",
       "         -12.5998, -23.5612, -26.7594, -16.6371, -26.6054, -23.0648, -22.8877,\n",
       "         -23.8496, -20.0774, -24.0744, -22.9121,  -2.4807, -19.3887, -11.8164,\n",
       "         -22.3309, -17.3634, -19.1429, -20.2219, -14.2905, -20.3359, -23.0794,\n",
       "         -20.1462, -24.5266, -22.5930, -19.3377, -25.4480,  -9.4408, -22.4818,\n",
       "         -16.3579, -21.5615, -19.8819, -25.0116, -12.7971, -12.0744, -23.7852,\n",
       "         -18.7592,   0.8238, -11.0621, -18.2845, -19.5459, -24.1541, -24.2154,\n",
       "         -25.8872, -21.9008, -16.1026, -21.5784, -20.6578,  -6.1335, -18.8481,\n",
       "         -22.8337, -21.8579, -23.6164, -22.2406, -28.6246, -18.4739, -26.7902,\n",
       "         -16.9647, -25.3859, -24.9407, -24.4306, -24.1991, -22.9167, -20.0317,\n",
       "         -26.5524, -17.5548, -21.0607, -23.2919, -18.1766, -23.9506, -18.8374,\n",
       "         -21.6022, -21.4338, -21.4863, -11.1924, -26.4048, -17.2814, -15.1685,\n",
       "         -23.4071, -22.8309, -20.4502, -21.2521, -19.4028, -16.4224, -20.4409,\n",
       "         -19.3038, -16.7212, -17.3631, -24.5391,   2.1417, -20.3912, -18.4178,\n",
       "         -23.5301, -17.0470, -12.6419, -15.5773, -14.6862, -25.0793, -21.8019,\n",
       "         -24.4196, -22.0690, -25.1862, -15.3728, -25.1884, -22.1829, -23.5997,\n",
       "         -18.1878, -21.7068, -23.4318, -26.7973, -27.1495, -18.7603, -19.6326,\n",
       "         -25.0286,  -9.0379, -25.7788, -17.8988, -25.7569, -25.7411, -22.5610,\n",
       "         -12.6199, -23.9440, -14.8643, -22.5248, -21.4061, -23.1507,  17.4942,\n",
       "         -22.7514, -26.8153, -13.0433, -12.1457, -22.6461, -20.0856, -18.0906,\n",
       "         -19.7119, -24.4252, -19.9856, -21.7139, -24.4405, -23.6397, -18.4757,\n",
       "         -15.6047, -25.4368, -19.5303, -28.1556, -18.9408, -20.3356, -25.7982,\n",
       "         -23.4504, -16.9653, -16.1218, -18.4178, -25.7860, -21.9723, -21.7894,\n",
       "          -8.6987, -17.3560, -20.7377, -17.3012, -20.3774, -24.4230, -18.8590,\n",
       "         -26.5140,  -6.2242, -13.0583, -11.9160, -21.7061, -22.0819, -22.9666,\n",
       "         -22.2763, -17.8175, -16.4063, -19.9665, -13.5852, -12.9508, -23.1034,\n",
       "         -21.2431, -24.8699, -25.1512, -20.8592, -22.5826, -25.5124,  -8.1682,\n",
       "         -18.9554, -22.1430, -22.5504, -18.0691,  -4.5743, -20.4322,  -4.4820,\n",
       "         -21.6073, -15.9539, -24.8053, -23.9282, -22.3961, -15.8227,  -9.2886,\n",
       "         -19.7829, -23.2114, -14.5429, -20.2144, -25.8651, -24.1508, -18.2522,\n",
       "          -6.8467, -11.4119, -20.7963, -14.9341, -21.8935, -16.7742, -17.0048,\n",
       "         -17.3015, -25.3898,   1.3441, -16.9749, -24.3564, -24.9757, -24.5876,\n",
       "         -16.7543, -18.6420, -21.3093, -21.5191, -14.9125, -18.4552, -17.1668,\n",
       "         -14.8798, -22.2668, -18.6444, -23.7497, -21.0134, -25.1739,  -9.4336,\n",
       "         -13.8729, -21.9155, -27.9283, -20.5024, -14.8408, -20.9832, -17.7305,\n",
       "         -19.0464, -13.8942,  -9.9204, -24.3131, -26.1145, -25.6701, -23.5365,\n",
       "         -26.4348, -20.6922, -19.3472, -24.5282, -21.4050, -21.4889, -23.4716,\n",
       "         -19.7003, -28.3593, -13.9487, -24.8254, -21.1090, -12.7611, -25.4278,\n",
       "         -25.3961, -13.7521,  -0.5377, -25.8655, -16.7995, -19.1447, -19.0372,\n",
       "         -17.0558,  -7.1059, -25.7749, -26.9635, -17.9941, -19.2503, -23.5860,\n",
       "         -20.6039,  -2.4482, -23.5246, -17.9213, -17.9557, -15.9924, -21.8184,\n",
       "         -18.1847, -20.7926, -18.8834, -24.8185, -19.1025, -18.1249, -25.3873,\n",
       "         -21.2765, -21.9673, -26.1348, -25.4144, -14.5277, -22.3847, -23.6665,\n",
       "         -13.5664, -16.7157, -24.0178, -22.1195, -14.7745,  -5.6019, -19.9984,\n",
       "         -12.9906, -23.2692, -26.2925, -15.6188, -17.7735, -26.6987, -16.8131,\n",
       "         -16.1741, -24.4179, -27.5663,  -0.6432, -25.7004, -20.6086, -25.5623,\n",
       "         -26.9640, -25.1319, -25.4968, -22.0465, -26.2540, -20.9392, -17.4596,\n",
       "          -4.9460, -24.3771, -17.5077, -15.2727, -25.7466, -18.4514, -21.4825,\n",
       "         -20.2468, -25.3763, -17.9593, -15.8728, -19.8786, -27.3514, -20.8209,\n",
       "         -20.0507, -20.0756, -23.9210, -21.9673,  -3.0168, -23.7280, -18.9953,\n",
       "         -22.8718,  -0.4533, -22.5554, -17.3760, -23.3480, -19.3468, -23.3912,\n",
       "         -21.2867, -22.2608, -26.4204, -22.5474, -21.8386, -20.9007, -26.3197,\n",
       "         -21.8071, -15.8211, -13.0152, -25.7633, -25.3823, -24.4932, -16.0280,\n",
       "         -21.2579, -19.5816, -21.7037, -13.7504, -17.4227, -22.1414, -11.1155,\n",
       "         -13.3185, -15.2854, -28.0811, -19.5217,  -8.5453, -23.5864, -23.6829,\n",
       "         -18.1079, -18.1423, -23.0157, -20.4502, -23.4119, -26.9949, -25.6525,\n",
       "         -20.6985, -24.7527, -20.0521, -20.4345, -24.3992, -24.3036, -22.0387,\n",
       "         -21.9870, -17.2879, -24.5985, -24.4014, -18.5741, -25.8397, -13.5777,\n",
       "         -22.1209, -18.1675, -17.5860,  -9.3689, -26.1480, -25.9048,  36.8983,\n",
       "         -23.0200, -24.7329, -24.5221, -23.8188, -19.5245, -20.0942, -11.0233,\n",
       "         -13.5722, -19.8614,  15.9812, -25.1688, -15.9889, -24.9210, -18.7997,\n",
       "         -25.0881, -17.8555, -16.3610, -21.7011], grad_fn=<SumBackward1>))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nll(dtest, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Covariance Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layers = [40, 40, 40]\n",
    "model = GaussianNeuralNet(1, 24, hidden_layers, 1, 12, covariance_type='full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1, loss: 0.7563\n",
      "epoch : 2, loss: -4.2190\n",
      "epoch : 3, loss: -5.4903\n",
      "epoch : 4, loss: -3.7717\n",
      "epoch : 5, loss: -7.2483\n",
      "epoch : 6, loss: -8.5624\n",
      "epoch : 7, loss: -9.5073\n",
      "epoch : 8, loss: -5.2787\n",
      "epoch : 9, loss: 3.0143\n",
      "epoch : 10, loss: -3.5749\n",
      "epoch : 11, loss: -5.9299\n",
      "epoch : 12, loss: -6.9604\n",
      "epoch : 13, loss: -8.0668\n",
      "epoch : 14, loss: -9.8617\n",
      "epoch : 15, loss: -10.1813\n",
      "epoch : 16, loss: -10.7907\n",
      "epoch : 17, loss: -11.1970\n",
      "epoch : 18, loss: -11.1961\n",
      "epoch : 19, loss: -11.5317\n",
      "epoch : 20, loss: -11.4176\n",
      "epoch : 21, loss: -11.6916\n",
      "epoch : 22, loss: -11.7135\n",
      "epoch : 23, loss: -12.1194\n",
      "epoch : 24, loss: -10.8978\n",
      "epoch : 25, loss: -9.2711\n",
      "epoch : 26, loss: -12.5709\n",
      "epoch : 27, loss: -12.3823\n",
      "epoch : 28, loss: -12.5376\n",
      "epoch : 29, loss: nan\n",
      "epoch : 30, loss: nan\n",
      "epoch : 31, loss: nan\n",
      "epoch : 32, loss: nan\n",
      "epoch : 33, loss: nan\n",
      "epoch : 34, loss: nan\n",
      "epoch : 35, loss: nan\n",
      "epoch : 36, loss: nan\n",
      "epoch : 37, loss: nan\n",
      "epoch : 38, loss: nan\n",
      "epoch : 39, loss: nan\n",
      "epoch : 40, loss: nan\n",
      "epoch : 41, loss: nan\n",
      "epoch : 42, loss: nan\n",
      "epoch : 43, loss: nan\n",
      "epoch : 44, loss: nan\n",
      "epoch : 45, loss: nan\n",
      "epoch : 46, loss: nan\n",
      "epoch : 47, loss: nan\n",
      "epoch : 48, loss: nan\n",
      "epoch : 49, loss: nan\n",
      "epoch : 50, loss: nan\n",
      "epoch : 51, loss: nan\n",
      "epoch : 52, loss: nan\n",
      "epoch : 53, loss: nan\n",
      "epoch : 54, loss: nan\n",
      "epoch : 55, loss: nan\n",
      "epoch : 56, loss: nan\n",
      "epoch : 57, loss: nan\n",
      "epoch : 58, loss: nan\n",
      "epoch : 59, loss: nan\n",
      "epoch : 60, loss: nan\n",
      "Learning finished!\n"
     ]
    }
   ],
   "source": [
    "train(model, X_batches, Y_batches, num_epochs=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(nan, grad_fn=<MeanBackward0>),\n",
       " tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "        grad_fn=<MeanBackward1>),\n",
       " tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "        grad_fn=<StdBackward1>))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtest = model(X_test)\n",
    "mape(dtest, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(nan, grad_fn=<PowBackward0>),\n",
       " tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "        grad_fn=<MeanBackward1>),\n",
       " tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "        grad_fn=<StdBackward1>))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse(dtest, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(nan, grad_fn=<MeanBackward0>),\n",
       " tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan], grad_fn=<NegBackward>))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nll(dtest, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Low Rank Covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layers = [40, 40, 40]\n",
    "model = GaussianNeuralNet(1, 24, hidden_layers, 1, 12, covariance_type='low-rank',rank=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1, loss: -3.1063\n",
      "epoch : 2, loss: -10.3741\n",
      "epoch : 3, loss: -13.1426\n",
      "epoch : 4, loss: -13.9573\n",
      "epoch : 5, loss: -15.1143\n",
      "epoch : 6, loss: -15.5392\n",
      "epoch : 7, loss: -16.3491\n",
      "epoch : 8, loss: -16.2735\n",
      "epoch : 9, loss: -16.6232\n",
      "epoch : 10, loss: -17.3809\n",
      "epoch : 11, loss: -17.5345\n",
      "epoch : 12, loss: -17.5058\n",
      "epoch : 13, loss: -17.8436\n",
      "epoch : 14, loss: -18.9384\n",
      "epoch : 15, loss: -18.8944\n",
      "epoch : 16, loss: -19.0581\n",
      "epoch : 17, loss: -18.9205\n",
      "epoch : 18, loss: -18.8942\n",
      "epoch : 19, loss: -19.5809\n",
      "epoch : 20, loss: -19.2164\n",
      "epoch : 21, loss: -19.2544\n",
      "epoch : 22, loss: -19.8083\n",
      "epoch : 23, loss: -20.0510\n",
      "epoch : 24, loss: -20.0107\n",
      "epoch : 25, loss: -19.8694\n",
      "epoch : 26, loss: -20.0495\n",
      "epoch : 27, loss: -19.5497\n",
      "epoch : 28, loss: -20.2489\n",
      "epoch : 29, loss: -20.0864\n",
      "epoch : 30, loss: -19.4829\n",
      "epoch : 31, loss: -20.6986\n",
      "epoch : 32, loss: -20.9322\n",
      "epoch : 33, loss: -20.8545\n",
      "epoch : 34, loss: -20.9490\n",
      "epoch : 35, loss: -20.9148\n",
      "epoch : 36, loss: -21.2406\n",
      "epoch : 37, loss: -20.3568\n",
      "epoch : 38, loss: -20.9078\n",
      "epoch : 39, loss: -20.9650\n",
      "epoch : 40, loss: -20.8746\n",
      "epoch : 41, loss: -20.6588\n",
      "epoch : 42, loss: -21.4458\n",
      "epoch : 43, loss: -21.1604\n",
      "epoch : 44, loss: -21.7034\n",
      "epoch : 45, loss: -15.6009\n",
      "epoch : 46, loss: -18.5610\n",
      "epoch : 47, loss: -19.6082\n",
      "epoch : 48, loss: -19.2906\n",
      "epoch : 49, loss: -20.1397\n",
      "epoch : 50, loss: -20.4358\n",
      "epoch : 51, loss: -20.1249\n",
      "epoch : 52, loss: -20.2869\n",
      "epoch : 53, loss: -20.5119\n",
      "epoch : 54, loss: -20.7762\n",
      "epoch : 55, loss: -20.3766\n",
      "epoch : 56, loss: -20.6843\n",
      "epoch : 57, loss: -21.2414\n",
      "epoch : 58, loss: -20.8194\n",
      "epoch : 59, loss: -21.2721\n",
      "epoch : 60, loss: -21.1434\n",
      "Learning finished!\n"
     ]
    }
   ],
   "source": [
    "train(model, X_batches, Y_batches, num_epochs=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0704, grad_fn=<MeanBackward0>),\n",
       " tensor([0.0425, 0.0565, 0.0779, 0.0767, 0.0670, 0.0730, 0.0723, 0.0661, 0.0783,\n",
       "         0.0890, 0.0793, 0.0660], grad_fn=<MeanBackward1>),\n",
       " tensor([0.0390, 0.0458, 0.0601, 0.0615, 0.0585, 0.0618, 0.0592, 0.0518, 0.0590,\n",
       "         0.0708, 0.0636, 0.0581], grad_fn=<StdBackward1>))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtest = model(X_test)\n",
    "mape(dtest, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0981, grad_fn=<PowBackward0>),\n",
       " tensor([0.0032, 0.0058, 0.0122, 0.0102, 0.0078, 0.0101, 0.0102, 0.0081, 0.0126,\n",
       "         0.0171, 0.0110, 0.0072], grad_fn=<MeanBackward1>),\n",
       " tensor([0.0051, 0.0092, 0.0207, 0.0175, 0.0150, 0.0195, 0.0206, 0.0135, 0.0217,\n",
       "         0.0311, 0.0160, 0.0117], grad_fn=<StdBackward1>))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse(dtest, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-23.0435, grad_fn=<MeanBackward0>),\n",
       " tensor([-17.6379, -23.9581, -25.1549, -20.1730, -17.7164, -24.7863, -20.4652,\n",
       "         -21.1271, -27.3322, -27.4133, -22.9483, -21.7543, -27.9186, -25.7621,\n",
       "         -25.4281, -22.6882, -26.1279, -17.0008, -22.4901, -26.4073, -26.5508,\n",
       "         -21.5318, -25.9927, -24.5449, -26.0411, -22.2509, -29.5128, -16.7363,\n",
       "         -28.9746, -25.0010, -20.5155, -27.7893, -17.7024, -24.9991, -26.9579,\n",
       "         -24.7814, -16.6304, -20.2253, -20.2465, -27.8986, -15.2050, -27.1029,\n",
       "         -26.3821, -20.8268, -20.2792, -26.2254, -21.0821, -18.0084, -25.5994,\n",
       "         -31.4710, -22.6845, -19.0172, -24.4441, -19.2032, -26.3612, -31.5482,\n",
       "         -15.8553, -21.7509, -20.3203, -28.4922, -22.5282, -29.5870, -18.9504,\n",
       "         -23.0070, -25.7244, -23.4984, -28.7966,  -6.2789, -25.3086, -28.8739,\n",
       "         -17.6948, -17.6005, -27.0394, -24.5648, -21.8508, -21.0781, -22.8716,\n",
       "         -27.1332,   2.8816, -25.1393, -24.0561, -25.8141, -29.9049, -26.4026,\n",
       "         -23.6077, -26.4553, -23.7202, -22.6631, -21.7860, -29.8087, -15.8491,\n",
       "         -26.2735, -25.8514, -23.0575, -24.4902, -30.3832, -23.6979, -17.5482,\n",
       "         -21.6286, -28.3624, -28.7815, -25.7225, -26.3583, -31.9043, -26.4307,\n",
       "         -25.5946, -25.7172, -17.0353, -24.2024, -24.8350, -20.8178, -24.9431,\n",
       "         -22.5100, -26.4294, -24.7422, -23.9882, -12.9929, -15.8208, -19.8084,\n",
       "         -25.2117, -22.5930, -20.5740, -29.5234, -24.6960, -31.6472, -25.3086,\n",
       "         -23.1834, -19.9162, -27.6349, -28.8285, -28.4636, -12.7734, -25.7332,\n",
       "         -18.7611, -28.2687, -26.8491, -29.4122, -19.0462, -18.7801, -28.0971,\n",
       "         -19.0228, -26.2412, -26.9923, -22.2018, -30.1556, -22.8857, -30.5994,\n",
       "         -17.3026, -20.2166, -24.4180, -25.8525, -22.1689, -19.2259, -18.0690,\n",
       "         -27.4015, -23.1234, -20.2327, -29.8669, -21.4310,  96.0346, -24.5027,\n",
       "         -22.5567, -26.9302, -26.4774, -26.3906, -20.0412, -29.5510, -25.1002,\n",
       "         -22.7014, -19.7975, -28.0886, -23.8093, -28.5384, -20.8481, -18.8941,\n",
       "         -18.3447, -28.0614, -24.8401, -26.9579, -23.3309, -26.2666, -24.7382,\n",
       "         -24.1474, -26.5120, -23.3208, -24.6246, -23.8067, -21.3024, -25.6415,\n",
       "         -28.9875, -25.0016, -25.6327, -29.4382, -27.3529, -24.3795, -26.2984,\n",
       "         -25.7951, -23.3600, -30.1894, -20.2924, -20.1197, -25.3871, -25.2804,\n",
       "         -21.9167, -25.7211, -24.8895, -26.2131, -23.5658, -22.4248, -26.2698,\n",
       "         -28.6074, -21.5963, -19.9345, -30.1039, -21.4561, -30.0823, -22.5039,\n",
       "         -25.2393, -20.8515, -24.3950, -22.6652, -23.4106, -25.4019, -26.9710,\n",
       "         -26.5072, -25.3841, -22.1280, -21.6339, -25.7266, -22.1757, -19.6955,\n",
       "         -23.4377, -22.1384, -12.6553, -23.3076, -26.5395, -23.3054, -21.0875,\n",
       "         -23.2503, -31.4037, -26.2468, -19.8222, -19.0986, -14.7205, -23.3684,\n",
       "         -21.9317, -22.8020, -29.3124, -25.0115, -26.9923, -26.4167, -28.1503,\n",
       "         -26.0238, -21.1451, -26.5283, -21.4788, -26.7212, -21.7643, -27.1828,\n",
       "         -27.2206, -27.1132, -22.4041, -25.3734, -28.6739, -23.3859, -23.3126,\n",
       "         -25.0711, -26.7381, -23.8675, -22.4965, -27.2215, -29.0361, -27.0371,\n",
       "         -22.3297, -27.5121, -21.2632, -21.8135, -22.1485, -28.3136, -22.4855,\n",
       "         -23.1776, -22.5938, -28.2835, -28.1910, -10.5618,  32.1904, -27.1332,\n",
       "         -29.9790, -18.8766, -24.1114, -26.0755, -31.5329, -29.2384, -20.0049,\n",
       "         -25.0686, -24.0297, -23.3184, -23.9406, -20.6012, -23.7794, -25.0785,\n",
       "         -24.9050, -25.9801, -27.3263, -26.5434, -24.4226, -17.2463, -25.3801,\n",
       "         -23.7844, -19.4284, -26.7369, -21.7991, -17.7498, -16.8102, -27.5236,\n",
       "         -25.1966,  -7.0913, -19.0036, -20.2093, -24.6495, -20.3247, -19.9413,\n",
       "         -25.6304, -23.8232, -20.8545, -15.3226, -26.9579, -23.7362,  33.7062,\n",
       "         -22.2073, -23.6684, -22.7874, -13.4275, -23.1631, -27.1432, -22.6897,\n",
       "         -26.4582, -22.4573, -29.4164, -23.1413, -24.1132, -26.3943, -24.4312,\n",
       "         -28.9025, -19.3273, -11.0319, -23.1986, -23.2343, -26.2104, -21.6734,\n",
       "         -30.2335, -21.9654, -22.6435, -25.3513, -27.4385, -27.5389, -27.2376,\n",
       "         -21.9652, -22.3738, -28.3419,  -2.9914, -22.7182, -20.0923, -22.3419,\n",
       "         -23.5064, -21.0591, -19.6528, -23.3190, -26.0362, -23.3669, -20.3424,\n",
       "         -21.5793, -24.0995, -24.2059, -20.4113, -31.2498, -22.6379, -23.0447,\n",
       "         -30.7417, -20.9732, -22.5084, -26.4502, -26.2802, -26.6854, -29.7565,\n",
       "         -26.4704, -22.6939, -27.2840, -17.1690, -27.0305, -25.2972, -25.1210,\n",
       "         -22.8399,  -9.8130, -26.7212, -24.8257, -27.4819, -22.0849, -22.8477,\n",
       "         -22.4066, -23.6270, -15.3443, -14.8066, -25.8006, -19.3442, -14.5111,\n",
       "         -18.2523, -27.1167, -28.6447, -21.6152, -23.2852, -21.9925, -22.5822,\n",
       "         -24.3062, -24.2356, -18.9793, -21.6978, -15.5886, -21.4115, -29.2057,\n",
       "         -25.1285, -12.6329, -20.5152, -21.5349, -17.6572, -24.6526, -29.3820,\n",
       "         -23.8403, -24.6368, -20.4370, -23.3292, -19.0129, -30.1810, -20.9619,\n",
       "           3.1704, -23.1672, -23.7791, -22.0968, -28.1402, -22.2774, -29.9765,\n",
       "         -27.7296, -15.3137, -17.5475, -20.7727, -14.6158, -19.8869, -23.7068,\n",
       "         -24.7788, -10.7306, -26.3112, -26.2702, -21.5772, -27.1062, -24.7748,\n",
       "         -26.2297, -26.3782, -27.7974, -20.2628, -20.9356, -30.8073, -15.6578,\n",
       "         -19.8971, -23.4747, -24.4310, -26.6434, -18.8614, -17.9236, -29.4428,\n",
       "         -25.2896, -21.9544, -22.7959, -21.9013, -26.4107, -22.3600, -22.5064,\n",
       "         -25.2275, -29.2250, -26.1720, -21.6340, -16.8503, -24.6666, -26.7588,\n",
       "         -23.9289, -10.6627, -24.1583, -26.2302, -22.6208, -23.8792, -26.3559,\n",
       "         -25.1871, -24.5028, -21.8704, -25.7704, -19.4077, -17.1801,  -5.4410,\n",
       "         -25.4608, -28.5818, -26.7921, -24.9378, -26.1339, -21.9960, -23.0300,\n",
       "         -23.4197, -25.6317, -25.8778, -30.0983, -26.9651, -24.4786, -23.1215,\n",
       "         -25.9224, -23.1843, -15.6639, -25.1138, -22.9407, -23.3621, -21.2472,\n",
       "         -24.7240, -24.7124, -28.8671, -21.6698, -26.5398, -22.8964, -18.8538,\n",
       "         -26.3627, -23.7469, -23.4871, -25.7858, -22.5631, -20.9879, -19.5655,\n",
       "         -23.1006, -15.7952, -15.1960, -24.3991, -12.5903, -26.9596, -13.3476,\n",
       "         -25.7472, -23.1163, -22.9904, -15.7468, -19.1097, -22.1365, -30.3450,\n",
       "         -21.7937, -28.7467, -25.3973, -19.1835, -28.1696, -21.3726, -23.5298,\n",
       "         -26.8139, -22.3888, -28.8917, -28.9949, -26.7343, -14.7656, -23.5140,\n",
       "         -29.5430, -25.9507, -26.7768, -24.1393, -26.9923, -28.0513, -24.5656,\n",
       "         -20.2594, -26.9620, -16.1827, -25.1911, -23.3427, -26.3945,  -1.7429,\n",
       "         -24.6691, -23.1471, -20.1215, -15.8307, -24.8657, -22.6276, -22.6185,\n",
       "         -24.2768, -21.8796, -23.0612, -26.1533, -29.6799, -22.7787, -24.8975,\n",
       "         -17.4893, -31.2207, -23.6256, -28.4372, -21.4669, -21.3659, -28.0035,\n",
       "         -22.0730, -21.7734, -17.0779, -13.3476, -24.1814, -25.0704, -25.9889,\n",
       "         -20.5993, -16.2027, -23.3209, -17.4358, -25.7749, -27.0479, -21.8049,\n",
       "         -30.3945, -15.2453, -18.7234, -24.3671, -25.1812, -22.2952, -22.6215,\n",
       "         -21.0135, -23.8226, -24.7756, -19.1052, -24.4674, -22.5134, -25.4969,\n",
       "         -21.3255, -27.3634, -25.5190, -23.2727, -22.8756, -26.3073,  -9.1000,\n",
       "         -20.0123, -26.3312, -25.2508, -25.0638, -17.0885, -25.6771,  35.2086,\n",
       "         -26.3536, -21.6669, -29.9322, -21.7787, -25.2872, -22.7742, -18.9407,\n",
       "         -13.6887, -23.6497, -16.9437, -20.5504, -26.4175, -25.2480, -19.3697,\n",
       "         -28.3406, -22.1638, -17.2079, -24.3826, -19.9604, -27.4123, -20.3519,\n",
       "         -20.0929, -21.8956, -26.8325, -20.8768, -26.4107, -27.0927, -21.8178,\n",
       "         -28.4153, -25.5272, -20.8850, -24.6777, -20.3826, -19.1923, -21.6951,\n",
       "         -23.7613, -26.4188, -25.2466, -27.0078, -23.5188, -22.9101, -28.7876,\n",
       "          -3.5200, -22.3208, -29.9485, -22.3901, -23.9522, -22.3562, -21.9276,\n",
       "         -23.1897, -19.5844, -19.2939, -25.7109, -30.0959, -23.8165, -24.9842,\n",
       "         -26.8166, -24.8812, -21.8849, -24.8422, -28.9194, -23.7417, -20.9861,\n",
       "         -20.7464, -27.3978, -20.1710, -28.6575, -13.1676, -22.9648, -26.3815,\n",
       "         -24.9122, -17.7050, -22.3232, -27.3053, -23.1792, -22.3709, -23.2261,\n",
       "         -21.5991, -24.6127, -24.2031, -29.6654, -26.4427, -19.2111, -25.5248,\n",
       "         -17.9917, -21.0481, -23.0848, -12.7430, -25.5784, -23.1520, -25.8105,\n",
       "         -19.3204, -26.0470, -24.9324, -24.5011, -24.4812, -21.3169, -24.1447,\n",
       "         -24.1783, -23.1776, -22.9551, -25.6332, -26.5944, -22.1925, -22.1379,\n",
       "         -19.3238, -19.7457, -23.4632, -18.9008, -21.0088, -28.9370, -23.5023,\n",
       "         -25.9356, -25.9743, -27.4113, -23.2530, -20.9006, -26.9448, -24.5632,\n",
       "         -21.5851, -26.6232, -28.8561, -25.0561, -25.6241, -24.7400, -28.6593,\n",
       "         -29.4728, -27.3397, -24.0141, -23.9374, -26.6424, -22.5161, -23.3580,\n",
       "           1.4465, -26.4860, -22.3093, -23.5129, -25.1499, -22.5527, -26.6570,\n",
       "         -23.5845, -23.8769, -24.8761, -18.9176, -23.2289, -27.8519, -20.9477,\n",
       "         -22.6968, -24.9273, -27.1218, -23.1776, -18.0492, -27.2175, -21.0082,\n",
       "         -24.6665, -17.9088, -23.9109, -18.6805, -27.8178, -29.2325, -24.7428,\n",
       "         -15.8266, -20.8242, -26.1850, -20.3962, -21.5833, -25.5254, -25.2585,\n",
       "         -29.3988, -15.4870, -20.5747, -29.5013, -24.3950, -21.7414, -22.0953,\n",
       "         -22.9617, -21.2853, -18.8832, -20.4139, -25.6025, -26.5903, -26.2304,\n",
       "         -22.9113, -20.4508, -29.8876, -26.7597,  -9.6254, -23.5064, -26.7895,\n",
       "         -23.6277, -21.6430, -25.8972, -23.4871, -24.5719, -25.7856, -22.7016,\n",
       "         -22.9874, -25.9076, -25.9952, -26.0910, -23.7283, -27.8393, -29.5571,\n",
       "         -19.7449, -23.0463, -24.8215, -28.5415, -22.3885, -26.4166, -20.4705,\n",
       "         -30.1868, -18.2399, -19.1305, -19.9317, -27.0934, -27.0197,  -4.3203,\n",
       "         -27.0803, -27.3322, -26.0362, -25.3701, -25.6110, -27.5247, -25.3054,\n",
       "         -24.0114, -23.3790, -19.4649, -25.7345, -19.2641, -24.9143, -22.7219,\n",
       "         -31.6782, -22.6748, -21.0832, -21.6983], grad_fn=<NegBackward>))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nll(dtest, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200, 872, 12])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtest.sample((200,)).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
