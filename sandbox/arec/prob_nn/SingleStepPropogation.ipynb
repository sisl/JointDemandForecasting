{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import sklearn as skl\n",
    "import time\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sys.path.append(\"../../../\")\n",
    "import CalibratedTimeseriesModels\n",
    "\n",
    "from CalibratedTimeseriesModels.utils import *\n",
    "from CalibratedTimeseriesModels.models.gnn import *\n",
    "from CalibratedTimeseriesModels.models.gmnn import *\n",
    "from CalibratedTimeseriesModels.models.blr import *\n",
    "from CalibratedTimeseriesModels.evaluators import ExplicitEvaluator\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "X_orig = torch.load(\"../../../datasets/processed/openEI/X_openei_011_subset_multitask.pt\")\n",
    "Y_orig = torch.load(\"../../../datasets/processed/openEI/Y_openei_011_subset_multitask.pt\")\n",
    "\n",
    "past_dims = 24\n",
    "fut_dims = 12\n",
    "\n",
    "X_train_orig, X_test_orig, Y_train_orig, Y_test_orig = electric_train_test_split(X_orig, Y_orig, \n",
    "                                                                                 disp_idx=past_dims+fut_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train_orig[1,:,:past_dims].reshape((-1,past_dims)).unsqueeze(-1).float()\n",
    "Y_train_full = Y_train_orig[1,:,:fut_dims].reshape((-1,fut_dims)).unsqueeze(-1).float()\n",
    "X_test = X_test_orig[1,:,:past_dims].reshape((-1,past_dims)).unsqueeze(-1).float()\n",
    "Y_test_full = Y_test_orig[1,:,:fut_dims].reshape((-1,fut_dims)).unsqueeze(-1).float()\n",
    "\n",
    "Y_train = Y_train_full[:,[0],:]\n",
    "Y_test = Y_test_full[:,[0],:]\n",
    "\n",
    "X_batches, Y_batches = batch(X_train, Y_train, batch_size = 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Step LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(0.0498), tensor([0.0498]), tensor([0.0485]))\n",
      "(tensor(0.0755), tensor([0.0755]), tensor([0.0436]))\n",
      "(tensor(0.0740), tensor([0.0055]), tensor([0.0109]))\n",
      "(tensor(0.0967), tensor([0.0094]), tensor([0.0108]))\n",
      "(tensor(-1.1512), tensor([-1.8127, -1.8539, -1.6309,  ..., -1.3570, -0.6578, -1.0662]))\n"
     ]
    }
   ],
   "source": [
    "lin_reg = BayesianLinearRegression(1, past_dims, 1, 1)\n",
    "lin_reg.fit(X_train, Y_train)\n",
    "dtest0 = lin_reg(X_test)\n",
    "for f in [mape, wape, rmse, rwse, nll]:\n",
    "    print(f(dtest0,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Step GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1, loss: 0.4372\n",
      "epoch : 11, loss: -1.3281\n",
      "epoch : 21, loss: -1.3911\n",
      "epoch : 31, loss: -1.4123\n",
      "epoch : 41, loss: -1.5076\n",
      "epoch : 51, loss: -1.4680\n",
      "epoch : 61, loss: -1.7906\n",
      "epoch : 71, loss: -1.6621\n",
      "epoch : 81, loss: -1.7677\n",
      "epoch : 91, loss: -1.9560\n",
      "epoch : 101, loss: -1.8870\n",
      "epoch : 111, loss: -1.7548\n",
      "epoch : 121, loss: -2.0011\n",
      "epoch : 131, loss: -1.9669\n",
      "epoch : 141, loss: -1.8842\n",
      "Learning finished!\n",
      "(tensor(0.0338, grad_fn=<MeanBackward0>), tensor([0.0338], grad_fn=<MeanBackward1>), tensor([0.0385], grad_fn=<StdBackward1>))\n",
      "(tensor(0.0452), tensor([0.0452]), tensor([0.0348]))\n",
      "(tensor(0.0573, grad_fn=<PowBackward0>), tensor([0.0033], grad_fn=<MeanBackward1>), tensor([0.0082], grad_fn=<StdBackward1>))\n",
      "(tensor(0.0754), tensor([0.0057]), tensor([0.0096]))\n",
      "(tensor(-1.6022, grad_fn=<MeanBackward0>), tensor([-3.1669, -3.1733, -2.9779,  ..., -0.1438, -0.7907, -1.7754],\n",
      "       grad_fn=<SumBackward1>))\n"
     ]
    }
   ],
   "source": [
    "hidden_layers = [40, 40, 40]\n",
    "ss_gnn = GaussianNeuralNet(1, past_dims, hidden_layers, 1, 1)\n",
    "train(ss_gnn, X_batches, Y_batches, num_epochs=150, learning_rate=.005)\n",
    "dtest_gnn = ss_gnn(X_test)\n",
    "for f in [mape, wape, rmse, rwse, nll]:\n",
    "    print(f(dtest_gnn,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Step GMNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1, loss: 0.7016\n",
      "epoch : 11, loss: -1.2484\n",
      "epoch : 21, loss: -1.4711\n",
      "epoch : 31, loss: -1.6136\n",
      "epoch : 41, loss: -1.6917\n",
      "epoch : 51, loss: -1.6859\n",
      "epoch : 61, loss: -1.7611\n",
      "epoch : 71, loss: -1.9041\n",
      "epoch : 81, loss: -1.8746\n",
      "epoch : 91, loss: -1.9587\n",
      "epoch : 101, loss: -1.8054\n",
      "epoch : 111, loss: -1.9890\n",
      "epoch : 121, loss: -1.8087\n",
      "epoch : 131, loss: -1.6759\n",
      "epoch : 141, loss: -2.0839\n",
      "Learning finished!\n",
      "(tensor(0.0343, grad_fn=<MeanBackward0>), tensor([0.0343], grad_fn=<MeanBackward1>), tensor([0.0377], grad_fn=<StdBackward1>))\n",
      "(tensor(0.0547), tensor([0.0547]), tensor([0.0334]))\n",
      "(tensor(0.0575, grad_fn=<PowBackward0>), tensor([0.0033], grad_fn=<MeanBackward1>), tensor([0.0068], grad_fn=<StdBackward1>))\n",
      "(tensor(0.0877), tensor([0.0077]), tensor([0.0088]))\n",
      "(tensor(-1.6733, grad_fn=<MeanBackward0>), tensor([-2.7498, -2.8066, -2.7430,  ..., -1.3938, -2.2997, -2.0622],\n",
      "       grad_fn=<NegBackward>))\n"
     ]
    }
   ],
   "source": [
    "hidden_layers = [40, 40, 40]\n",
    "ss_gmnn = GaussianMixtureNeuralNet(1, past_dims, hidden_layers, 1, 1, n_components=3)\n",
    "train(ss_gmnn, X_batches, Y_batches, num_epochs=150, learning_rate=.005)\n",
    "dtest_gmnn = ss_gmnn(X_test)\n",
    "for f in [mape, wape, rmse, rwse, nll]:\n",
    "    print(f(dtest_gmnn,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Step GLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1, loss: 0.9823\n",
      "epoch : 11, loss: -0.5173\n",
      "epoch : 21, loss: -1.0173\n",
      "epoch : 31, loss: -1.1674\n",
      "epoch : 41, loss: -1.4493\n",
      "epoch : 51, loss: -1.5867\n",
      "epoch : 61, loss: -1.6917\n",
      "epoch : 71, loss: -1.6981\n",
      "epoch : 81, loss: -1.8621\n",
      "epoch : 91, loss: -1.9247\n",
      "epoch : 101, loss: -2.0530\n",
      "epoch : 111, loss: -2.0541\n",
      "Learning finished!\n",
      "(tensor(0.0354, grad_fn=<MeanBackward0>), tensor([0.0354], grad_fn=<MeanBackward1>), tensor([0.0427], grad_fn=<StdBackward1>))\n",
      "(tensor(0.0453), tensor([0.0453]), tensor([0.0404]))\n",
      "(tensor(0.0619, grad_fn=<PowBackward0>), tensor([0.0038], grad_fn=<MeanBackward1>), tensor([0.0098], grad_fn=<StdBackward1>))\n",
      "(tensor(0.0780), tensor([0.0061]), tensor([0.0123]))\n",
      "(tensor(-1.3906, grad_fn=<MeanBackward0>), tensor([-3.4392, -3.5476, -3.3663,  ...,  1.6993, -2.8094, -2.4952],\n",
      "       grad_fn=<NegBackward>))\n"
     ]
    }
   ],
   "source": [
    "hidden_layers = [20, 20, 20]\n",
    "hidden_dim = 40\n",
    "ss_glstm = GaussianLSTM(1, hidden_dim, hidden_layers, 1, 1)\n",
    "train(ss_glstm, X_batches, Y_batches, num_epochs=120, learning_rate=.005)\n",
    "dtest_glstm = ss_glstm(X_test)\n",
    "for f in [mape, wape, rmse, rwse, nll]:\n",
    "    print(f(dtest_glstm,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Step GMM LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1, loss: 0.9227\n",
      "epoch : 11, loss: -1.1746\n",
      "epoch : 21, loss: -1.2586\n",
      "epoch : 31, loss: -1.3158\n",
      "epoch : 41, loss: -1.3251\n",
      "epoch : 51, loss: -1.3531\n",
      "epoch : 61, loss: -1.4063\n",
      "epoch : 71, loss: -1.4360\n",
      "epoch : 81, loss: -1.4693\n",
      "epoch : 91, loss: -1.5216\n",
      "epoch : 101, loss: -1.5683\n",
      "epoch : 111, loss: -1.5885\n",
      "Learning finished!\n",
      "(tensor(0.0791, grad_fn=<MeanBackward0>), tensor([0.0791], grad_fn=<MeanBackward1>), tensor([0.0913], grad_fn=<StdBackward1>))\n",
      "(tensor(0.1035), tensor([0.1035]), tensor([0.0848]))\n",
      "(tensor(0.1387, grad_fn=<PowBackward0>), tensor([0.0192], grad_fn=<MeanBackward1>), tensor([0.0408], grad_fn=<StdBackward1>))\n",
      "(tensor(0.1839), tensor([0.0338]), tensor([0.0506]))\n",
      "(tensor(-0.9125, grad_fn=<MeanBackward0>), tensor([-2.4752, -2.6291, -2.5837,  ..., -1.0152, -1.3710, -1.5033],\n",
      "       grad_fn=<NegBackward>))\n"
     ]
    }
   ],
   "source": [
    "hidden_layers = [20, 20, 20]\n",
    "hidden_dim = 40\n",
    "ss_gmmlstm = GaussianMixtureLSTM(1, hidden_dim, hidden_layers, 1, 1, n_components=2)\n",
    "train(ss_gmmlstm, X_batches, Y_batches, num_epochs=120, learning_rate=.005)\n",
    "dtest_gmmlstm = ss_gmmlstm(X_test)\n",
    "for f in [mape, wape, rmse, rwse, nll]:\n",
    "    print(f(dtest_gmmlstm,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Sampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_forward_nn(model, y, prediction_horizon, n_samples=1000):\n",
    "    \n",
    "    # initial step\n",
    "    sequence = [model(y).sample((n_samples,))] #(n_samples, B, 1)\n",
    "    \n",
    "    # append to front of input sequence\n",
    "    \n",
    "    for i in range(1, prediction_horizon):\n",
    "\n",
    "        # run through model\n",
    "        dist = model(y)\n",
    "        \n",
    "        # generate next time series\n",
    "        \n",
    "        # append latest samples to front of sequence (OPENEI DATA)\n",
    "        y = torch.cat((),)\n",
    "        \n",
    "    samples = torch.cat(sequence,2)\n",
    "    return samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_forward_lstm(model, y, n_samples=1000):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
