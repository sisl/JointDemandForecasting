{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import sklearn as skl\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import time\n",
    "import requests, zipfile, io\n",
    "\n",
    "from data_utils.read_csv import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download/Extract OpenEI Dataset\n",
    "\n",
    "https://openei.org/datasets/files/961/pub/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'https://openei.org/datasets/files/961/pub/EPLUS_TMY2_RESIDENTIAL_BASE.zip'\n",
    "pathdir = './raw/'\n",
    "\n",
    "if not os.path.exists(pathdir):\n",
    "    r = requests.get(dataset)\n",
    "    z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "    z.extractall(pathdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathdir = './raw/EPLUS_TMY2_RESIDENTIAL_BASE/'\n",
    "files = sorted([f for f in os.listdir(pathdir) if os.path.isfile(os.path.join(pathdir, f))])\n",
    "nfiles = len(files)\n",
    "chosenfiles = 10\n",
    "startidx = 5 # capture Big Delta, AK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./raw/EPLUS_TMY2_RESIDENTIAL_BASE/USA_AK_Big.Delta.702670_TMY2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 1/10 [00:01<00:10,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./raw/EPLUS_TMY2_RESIDENTIAL_BASE/USA_CA_Bakersfield.723840_TMY2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 2/10 [00:02<00:09,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./raw/EPLUS_TMY2_RESIDENTIAL_BASE/USA_FL_Tampa.722110_TMY2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 3/10 [00:03<00:08,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./raw/EPLUS_TMY2_RESIDENTIAL_BASE/USA_IN_Evansville.724320_TMY2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 4/10 [00:04<00:07,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./raw/EPLUS_TMY2_RESIDENTIAL_BASE/USA_MI_Grand.Rapids.726350_TMY2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 5/10 [00:06<00:06,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./raw/EPLUS_TMY2_RESIDENTIAL_BASE/USA_MT_Lewistown.726776_TMY2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 6/10 [00:07<00:04,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./raw/EPLUS_TMY2_RESIDENTIAL_BASE/USA_NV_Ely.724860_TMY2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████   | 7/10 [00:08<00:03,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./raw/EPLUS_TMY2_RESIDENTIAL_BASE/USA_OR_Eugene.726930_TMY2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 8/10 [00:09<00:02,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./raw/EPLUS_TMY2_RESIDENTIAL_BASE/USA_TN_Bristol.723183_TMY2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████ | 9/10 [00:10<00:01,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./raw/EPLUS_TMY2_RESIDENTIAL_BASE/USA_UT_Salt.Lake.City.725720_TMY2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 10/10 [00:12<00:00,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./raw/EPLUS_TMY2_RESIDENTIAL_BASE/USA_WY_Rock.Springs.725744_TMY2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [00:13,  1.22s/it]                        \n"
     ]
    }
   ],
   "source": [
    "load_features = ['DT', 'ELEC']\n",
    "options = {}\n",
    "Xs, Ys, filename = [], [], []\n",
    "loadnum = 0\n",
    "with tqdm.tqdm(total=chosenfiles) as pbar:\n",
    "    for i,file in enumerate(files):\n",
    "        if (i-startidx) % (nfiles//chosenfiles) is not 0:\n",
    "            continue\n",
    "        path = os.path.join(pathdir, file)\n",
    "        print(path)\n",
    "        df = read_load_csv(path,load_features)\n",
    "        data, labels = df_to_numpy(df)\n",
    "        x, y = encode_features(data, options)\n",
    "        Xs.append(x)\n",
    "        Ys.append(y)\n",
    "        filename.append(file)\n",
    "        loadnum += 1\n",
    "        pbar.update(1)\n",
    "X = np.vstack(Xs)\n",
    "Y = np.vstack(Ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save X and Y as torch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "X_torch = torch.tensor(X)\n",
    "Y_torch = torch.tensor(Y)\n",
    "\n",
    "output_dir = 'processed'\n",
    "postfix = '_openei_%03d_subset_multitask.pt' %(loadnum)\n",
    "if not os.path.exists(output_dir):\n",
    "        os.mkdir(output_dir)   \n",
    "        \n",
    "torch.save(X_torch, os.path.join(output_dir, 'X'+postfix))\n",
    "torch.save(Y_torch, os.path.join(output_dir, 'Y'+postfix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.1,random_state=1)\n",
    "X_train_red, X_cv, Y_train_red, Y_cv = train_test_split(X_train, Y_train, train_size = 0.7/0.9,random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "models, r2, mse = [], [], []\n",
    "for col in range(Y.shape[1]):\n",
    "    reg = LinearRegression().fit(X_train, Y_train[:,col])\n",
    "    models.append(reg)\n",
    "    r2.append(reg.score(X_test,Y_test[:,col]))\n",
    "    mse.append(mean_squared_error(Y_test[:,col],reg.predict(X_test)))\n",
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validated Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "models, r2, mse = [], [], []\n",
    "for col in range(Y.shape[1]):\n",
    "    reg = LassoCV(cv=5,random_state=0).fit(X_train, Y_train[:,col])\n",
    "    models.append(reg)\n",
    "    r2.append(reg.score(X_test,Y_test[:,col]))\n",
    "    mse.append(mean_squared_error(Y_test[:,col],reg.predict(X_test)))\n",
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "models, r2, mse = [], [], []\n",
    "min_depth, max_depth = 2,15\n",
    "\n",
    "with tqdm.tqdm(total=Y.shape[1]*(max_depth+1-min_depth)) as pbar:\n",
    "    for col in range(Y.shape[1]):\n",
    "\n",
    "        # hyperparameter search over depth\n",
    "        model_options, cv_mse = [], []\n",
    "        for d in range(min_depth,max_depth+1):\n",
    "            reg = DecisionTreeRegressor(max_depth=d).fit(X_train_red, Y_train_red[:,col])\n",
    "            model_options.append(reg)\n",
    "            cv_mse.append(mean_squared_error(Y_cv[:,col],reg.predict(X_cv)))\n",
    "            pbar.update(1)\n",
    "            \n",
    "        best_cv_idx = np.array(cv_mse).argmax()\n",
    "        print(\"Best tree depth for column %d: %d\" %(col,best_cv_idx+3))\n",
    "        reg = model_options[best_cv_idx]\n",
    "        models.append(reg)\n",
    "        r2.append(reg.score(X_test,Y_test[:,col]))\n",
    "        mse.append(mean_squared_error(Y_test[:,col],reg.predict(X_test)))\n",
    "\n",
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import itertools\n",
    "\n",
    "models, r2, mse = [], [], []\n",
    "#hyperparameters\n",
    "kernels = ['poly','rbf']\n",
    "Cs = [0.2,1.0,5.0] #[0.1,0.2,0.5,1.0,2.0,5.0,10.0]\n",
    "\n",
    "with tqdm.tqdm(total=Y.shape[1]*len(kernels)*len(Cs)) as pbar:\n",
    "    for col in range(Y.shape[1]):\n",
    "\n",
    "        # hyperparameter search\n",
    "        model_options, cv_mse = [], []\n",
    "        for kernel, C in list(itertools.product(kernels, Cs)):\n",
    "            reg = SVR(kernel=kernel, C=C).fit(X_train_red, Y_train_red[:,col])\n",
    "            model_options.append(reg)\n",
    "            cv_mse.append(mean_squared_error(Y_cv[:,col],reg.predict(X_cv)))\n",
    "            pbar.update(1)\n",
    "            \n",
    "        best_cv_idx = np.array(cv_mse).argmax()\n",
    "        print(\"Best hyperparameter options for column %d: %s\" %(col,list(itertools.product(kernels, Cs))[best_cv_idx]))\n",
    "        reg = model_options[best_cv_idx]\n",
    "        models.append(reg)\n",
    "        r2.append(reg.score(X_test,Y_test[:,col]))\n",
    "        mse.append(mean_squared_error(Y_test[:,col],reg.predict(X_test)))\n",
    "\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "models, r2, mse = [], [], []\n",
    "#hyperparameters\n",
    "parameters = [\n",
    "  {'C': [1, 10, 100, 1000], 'kernel': ['poly']},\n",
    "  {'C': [1, 10, 100, 1000], 'gamma': [0.1, 0.01, 0.001], 'kernel': ['rbf']},\n",
    " ]\n",
    "parameters = [\n",
    "  {'C': [1, 10], 'gamma': [0.1, 0.01], 'kernel': ['rbf']},\n",
    " ]\n",
    "with tqdm.tqdm(total=Y.shape[1]) as pbar:\n",
    "    for col in range(Y.shape[1]):\n",
    "\n",
    "        # hyperparameter search\n",
    "        reg = SVR()\n",
    "        clf = GridSearchCV(reg, parameters, scoring='neg_mean_squared_error', cv=5, n_jobs=-1)\n",
    "        clf.fit(X_train, Y_train[:,col])\n",
    "        \n",
    "        pbar.update(1)\n",
    "        \n",
    "        models.append(clf)\n",
    "        r2.append(clf.score(X_test,Y_test[:,col]))\n",
    "        mse.append(mean_squared_error(Y_test[:,col],clf.predict(X_test)))\n",
    "\n",
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Shallow Neural Nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {'hidden_layer_sizes': [(10,10),(20,20),(30,30),(20,20,20),(30,30,30)]}\n",
    "models, r2, mse = [], [], []\n",
    "with tqdm.tqdm(total=Y.shape[1]) as pbar:\n",
    "    for col in range(Y.shape[1]):\n",
    "        mlp = MLPRegressor(learning_rate='adaptive')\n",
    "        reg = GridSearchCV(mlp, parameters, scoring='neg_mean_squared_error', cv=5, n_jobs=-1)\n",
    "        reg.fit(X_train, Y_train[:,col])\n",
    "        models.append(reg)\n",
    "        r2.append(reg.score(X_test,Y_test[:,col]))\n",
    "        mse.append(mean_squared_error(Y_test[:,col],reg.predict(X_test)))\n",
    "        pbar.update(1)\n",
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Net w/ Shared Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "models, r2, mse = [], [], []\n",
    "parameters = {'hidden_layer_sizes': [(10,10),(20,20),(30,30),(50,50),(100,100),(20,20,20),(30,30,30),\n",
    "                                     (50,50,50),(20,20,20,20),(30,30,30,30)]}\n",
    "#parameters = {'hidden_layer_sizes': [(30,30,30)]}\n",
    "mlp = MLPRegressor(learning_rate='adaptive')\n",
    "reg = GridSearchCV(mlp,parameters, scoring='neg_mean_squared_error', cv=5, n_jobs=-1)\n",
    "reg.fit(X_train, Y_train)\n",
    "models.append(reg)\n",
    "r2.append(reg.score(X_test,Y_test))\n",
    "\n",
    "Y_test_pred = reg.predict(X_test)\n",
    "for col in range(Y_test.shape[1]):\n",
    "    mse.append(mean_squared_error(Y_test_pred[col], Y_test[col]))\n",
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dirty MTL Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$W = P+Q$, $W$ of size (input features, output tasks)\n",
    "\n",
    "$Q$ is element-wise sparse and is regularized with $\\lVert Q \\rVert_1$\n",
    "\n",
    "$P$ is low-rank and is regularized with $\\lVert P \\rVert_*$ or row-wise group-sparse, regularized with $\\lVert P \\rVert_{1,q}$\n",
    "\n",
    "The p,q norm takes the q-norm of each row (single feature to multiple tasks), then the p-norm over q-norms.\n",
    "\n",
    "The * norm is the sum of singular values obtained from SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robust MTL Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$W = P+Q$, $W$ of size (input features, output tasks)\n",
    "\n",
    "$Q$ is column-wise group sparse (against outlier tasks) and is regularized with $\\lVert Q^T \\rVert_{1,q}$\n",
    "\n",
    "$P$ is low-rank and is regularized with $\\lVert P \\rVert_*$ or row-wise group-sparse, regularized with $\\lVert P \\rVert_{1,q}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
